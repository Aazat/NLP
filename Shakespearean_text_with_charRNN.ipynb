{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gIPSHnKcO-E-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCZyAIxijakG"
      },
      "source": [
        "# Downloading and loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnyLvF96PMOE",
        "outputId": "156261f7-61da-4545-ab2a-3aa57976e9e0"
      },
      "outputs": [],
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = keras.utils.get_file(\"shakespear.txt\", shakespeare_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YcAzqchtPccg"
      },
      "outputs": [],
      "source": [
        "with open(filepath) as f:\n",
        "  shakespear_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "otHoA9GIPi-y",
        "outputId": "f7ce4d03-1a0a-4edb-bde2-42f983a9fbb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shakespear_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZRjVfz-jfLv"
      },
      "source": [
        "# Tokenizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9mq1m3gOPm6_"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level= True)\n",
        "tokenizer.fit_on_texts([shakespear_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer.word_index\n",
        "import json\n",
        "with open('word_dict.json', 'w') as file:\n",
        "    json.dump(tokenizer.word_index, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('index_word.json', 'w') as file:\n",
        "    json.dump(tokenizer.index_word, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IhRYHw2Wb9O",
        "outputId": "781d474f-5d58-4ca9-9a7f-8cb86aa68697"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[]]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpsrWgFbYK5j",
        "outputId": "9be10692-2be3-40d4-a109-f2504e5df781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39 1115394\n"
          ]
        }
      ],
      "source": [
        "max_id = len(tokenizer.word_index)\n",
        "dataset_size = sum([_ for x, _ in tokenizer.word_counts.items()])\n",
        "print(max_id, dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NGmeuUeLZpSa"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuR9X8JYZGda",
        "outputId": "bc7c6db3-d7a9-46d0-e265-7fecfc425f9e"
      },
      "outputs": [],
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespear_text])) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QLxPDxFZsY5",
        "outputId": "906fff08-4e45-44d5-81b7-ef496297ece4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([19,  5,  8, ..., 20, 26, 10])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2meFiH1Fjkby"
      },
      "source": [
        "# Creating training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iopiHcz3Z4Zo"
      },
      "outputs": [],
      "source": [
        "train_size = dataset_size * 10 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v-_z4VncFYn",
        "outputId": "37451d34-2877-469f-8eef-2627d6e80d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111539\n"
          ]
        }
      ],
      "source": [
        "print(train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZiLb2w0cokb",
        "outputId": "ab156c0a-f132-4802-a61b-94111ef47808"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS_bZDvQjmyM"
      },
      "source": [
        "# Windowing training dataset (truncated backpropogation through time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GMl0WbRfg2W1"
      },
      "outputs": [],
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(window_length, shift= 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HBSo7o5o5FB"
      },
      "source": [
        "## Converting nested dataset to dataset of tensors and batching them using flat_map()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dS32xWRLkIuz"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.flat_map(lambda window : window.batch(window_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u9lhoBNopW9s"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows : (windows[:, :-1], windows[ : , 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S44vkzNgs4Ws"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YV1SwsGFvghA"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIW3uKqHxkOA"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmTA9EDPwQz6",
        "outputId": "cda42f6c-1fa3-4495-e8b6-2907d247f499"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "                          keras.layers.GRU(128, return_sequences=True, input_shape= [None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
        "                          keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "                          keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
        "                          ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "DbYfZsuqyRHj",
        "outputId": "ee5513e6-5dd1-48d5-f42a-4e9a6e7aab79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "3483/3483 [==============================] - 663s 189ms/step - loss: 1.7449\n",
            "Epoch 2/20\n",
            "3483/3483 [==============================] - 689s 197ms/step - loss: 1.4674\n",
            "Epoch 3/20\n",
            "3483/3483 [==============================] - 729s 209ms/step - loss: 1.4053\n",
            "Epoch 4/20\n",
            "3483/3483 [==============================] - 694s 199ms/step - loss: 1.3711\n",
            "Epoch 5/20\n",
            "3483/3483 [==============================] - 723s 207ms/step - loss: 1.3469\n",
            "Epoch 6/20\n",
            "3483/3483 [==============================] - 707s 203ms/step - loss: 1.3286\n",
            "Epoch 7/20\n",
            "3483/3483 [==============================] - 706s 202ms/step - loss: 1.3149\n",
            "Epoch 8/20\n",
            "3483/3483 [==============================] - 697s 200ms/step - loss: 1.3036\n",
            "Epoch 9/20\n",
            "3483/3483 [==============================] - 688s 197ms/step - loss: 1.2935\n",
            "Epoch 10/20\n",
            "3483/3483 [==============================] - 675s 193ms/step - loss: 1.2865\n",
            "Epoch 11/20\n",
            "3483/3483 [==============================] - 681s 195ms/step - loss: 1.2801\n",
            "Epoch 12/20\n",
            "3483/3483 [==============================] - 704s 202ms/step - loss: 1.2749\n",
            "Epoch 13/20\n",
            "3483/3483 [==============================] - 717s 206ms/step - loss: 1.2696\n",
            "Epoch 14/20\n",
            "3483/3483 [==============================] - 697s 200ms/step - loss: 1.2657\n",
            "Epoch 15/20\n",
            "3483/3483 [==============================] - 727s 208ms/step - loss: 1.2619\n",
            "Epoch 16/20\n",
            "3483/3483 [==============================] - 740s 212ms/step - loss: 1.2587\n",
            "Epoch 17/20\n",
            "3483/3483 [==============================] - 740s 212ms/step - loss: 1.2559\n",
            "Epoch 18/20\n",
            "3483/3483 [==============================] - 744s 213ms/step - loss: 1.2533\n",
            "Epoch 19/20\n",
            "3483/3483 [==============================] - 727s 208ms/step - loss: 1.2509\n",
            "Epoch 20/20\n",
            "3483/3483 [==============================] - 715s 205ms/step - loss: 1.2480\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs= 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hOh7Huvkye0x"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    X = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
        "    return tf.one_hot(X, max_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_new = preprocess([\"How are yo\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 10, 39)\n"
          ]
        }
      ],
      "source": [
        "Y_pred_array = model.predict(X_new)\n",
        "print(Y_pred_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In the one-hot encoding scheme, it's actually the indices between 0-39 (+1 for predictions) represents characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_char(text):\n",
        "    X_new = preprocess([text])\n",
        "    y_pred_array = model.predict(X_new)\n",
        "    max_ind = np.argmax(y_pred_array, axis=2)\n",
        "    return tokenizer.sequences_to_texts(max_ind + 1)[0][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_char(\"How are yo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'g'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_char(\"Somethin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n\n",
            "d\n",
            "g\n",
            "a\n",
            "d\n",
            "y\n"
          ]
        }
      ],
      "source": [
        "tests = [\"Dow\", \"Ol\", \"Trainin\", \"Difficul\", \"Har\", \"Dignit\"]\n",
        "for text in tests:\n",
        "    print(predict_char(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating texts from generated characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "something, whose p\n"
          ]
        }
      ],
      "source": [
        "next_window = 10\n",
        "initial_text = \"somethin\"\n",
        "for i in range(next_window):\n",
        "    next_char = predict_char(initial_text)\n",
        "    initial_text += next_char\n",
        "print(initial_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'something, whose p'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_further(input_text, window= 10):\n",
        "    initial_text = input_text\n",
        "    for i in range(window):\n",
        "        next_char = predict_char(initial_text)\n",
        "        initial_text += next_char\n",
        "    return initial_text\n",
        "predict_further(\"somethin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the ultime to the p'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_further(\"the ultim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'something, whose parts and present\\nto the people'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_further(\"somethin\", 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'futus:\\ni have '"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_further(\"futu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Randomly picking next_char with tf.random.categorical()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_prob = model.predict(X_new)[0, -1:, : ]\n",
        "    log_norm = tf.math.log(y_prob)/temperature\n",
        "    char_id = tf.random.categorical(log_norm, num_samples= 1) +1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_char(\"giv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for i in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the man i am good master?\n",
            "\n",
            "coriolanus:\n",
            "i would he h\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"t\", temperature=0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "throke thee\n",
            "since that may do that a friendn hath f\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "with thy hand: i have deserved thee here,\n",
            "thou hast \n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"w\", n_chars=51,temperature=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "welt\n",
            "beenefy! hilr! why hasham.\n",
            "\n",
            "secood;\n",
            "cike? no,?\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"w\", temperature=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wife is past of them. now, that is the\n",
            "truer i am marc\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"wife\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "husband mine with man:\n",
            "worthy name. sir.\n",
            "\n",
            "sicinius:\n",
            "there\n"
          ]
        }
      ],
      "source": [
        "print(complete_text(\"husband\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simpler way to batch by creating batches containing single window each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 39), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift= n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window : window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows : (windows[:, :-1], windows[ : , 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching for stateful RNN proper way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "print(len(encoded_parts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(32, None, 39), dtype=tf.float32, name=None), TensorSpec(shape=(32, None), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "    dataset = dataset.window(window_length, shift= n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window : window.batch(window_length))\n",
        "    datasets.append(dataset)\n",
        "\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *window : tf.stack(window))\n",
        "dataset = dataset.map(lambda windows : (windows[:, :-1], windows[ : , 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, batch_input_shape=[batch_size, None, max_id], \n",
        "    dropout= 0.2, recurrent_dropout= 0.2),\n",
        "    keras.layers.GRU(128, return_sequences= True, stateful= True, dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation= 'softmax'))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Callback to reset states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epochs_begin(self, epoch, logs):\n",
        "        self.model_reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Shakespearean text with charRNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "bbf1a8f77fd12945f7664f65bead548cc078d0d6fab61f2157992f446face408"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit (system)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
